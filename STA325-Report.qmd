---
title: "STA325 Case Study"
author: "Chase Mathis, Dillan Sant"
format: html
execute: 
  warning: false
  echo: false
editor: visual
---

## Introduction

Nobel Prize winning Physicist Richard Feynman said that turbulence was "the most important unsolved problem of classical physics". Although, we still unexpectedly feel bumps on airplanes and have trouble truly predicting where a Hurricane will strike land, we hope to add to the field of research studying how to model turbulent systems. We position our research in two key areas:

1.  Prediction
2.  Statistical Inference

Prediction is important in the context of this problem as data from turbulent systems is very computationally expensive to obtain. Therefore, experiments using data on the evolution turbulent systems need better, quicker, cheaper methods to get data.

Statistical Inference is also important as it can help elucidate the connections between turbulence and initial settings. We hope to build a model that gives clarity on this as well.

## Methodology

```{r}
#| echo: false
library(tidyverse) # datawrangling
library(tidymodels) #modeling steps
library(knitr) # nice output
library(boot) # cv
library(lmvar) # CV
library(patchwork) # plots
library(splines)
library(tree) # trees

ggplot2::theme_set(ggplot2::theme_minimal())

train <- read_csv("data/data-train.csv")
test <- read_csv("data/data-test.csv")
#make a validation set for comparing models
set.seed(123)
split <- initial_split(train)
train <- training(split)
validation <- testing(split)
```

```{r}

## mutate data so that we get central moments
centralTrain <- train |> 
  mutate(mean = R_moment_1,
         variance = R_moment_2 - R_moment_1^2,
         skewness = R_moment_3 -3*R_moment_1*R_moment_2 + 2*R_moment_1^3,
         kurtosis = R_moment_4 - 4*R_moment_1*R_moment_3 + 6*R_moment_2*R_moment_1^2 - 3*R_moment_1^4) |> 
  dplyr::select(-contains("R_"))

centralVal <- validation |> 
  mutate(mean = R_moment_1,
         variance = R_moment_2 - R_moment_1^2,
         skewness = R_moment_3 -3*R_moment_1*R_moment_2 + 2*R_moment_1^3,
         kurtosis = R_moment_4 - 4*R_moment_1*R_moment_3 + 6*R_moment_2*R_moment_1^2 - 3*R_moment_1^4) |> 
  dplyr::select(-contains("R_"))
```

We are given `r nrow(train) + nrow(test) + nrow(validation)` data points, but we only use `r nrow(train) + nrow(validation)` data points for training our model as to not overfit our model. We will examine numerous different types of models including:

-   A Simple Linear Regression Model
-   More Complex Non-Linear Regression Model
-   Tree Based Model
-   Boosted Tree Model

In addition to our splitting of data for training and testing, we further split our training data where `r nrow(train)` data points are used for training and `r nrow(validation)` data points are used for selecting our best model. When fitting the models mentioned above, we benchmark their predictive performance on the validation set.

While cross-validation is a common technique to estimate the true test-error, we believe that it would be too computationally expensive to fit many different folds of all of the models we wish to predict. This is a shortcoming, and provides for future investigation.

The data has `4` response variables and `3` input variables. Each output variable is a raw moment of a final turbulent distribution. We've converted the raw moments to central moments so as for better interpretability. We keep the first raw moment, however, as the first central moment is 0.

As there are only three input variables, we will stray away from methods which penalize models with high numbers of correlated predictors i.e. (lasso/ridge regression). Our model is already sparse so there is there is no need to make it more sparse and there is low risk that the predictors are highly correlated.

#### Quick Transformations for our Input Variables

```{r}
centralTrain |> 
  count(Re) |> 
  bind_cols(centralTrain |> count(Fr)) |> 
  kable(col.names = c("Reynolds Number", "Count", "Froude Number", "Count"))
```

Reynolds and Froude numbers are non-continuous as there is only three buckets for the numbers. Therefore, we are going to make them a categorical variable to help interpretability.

```{r}
centralTrain <- centralTrain |> 
  mutate(Fr = case_when(
    Fr < 0.3 ~ "Low",
    Fr == 0.3 ~ "Medium",
    Fr == Inf ~ "Infinity"
  )) |> 
  mutate(Fr = factor(Fr, levels = c("Low", "Medium", "Infinity")))

centralTrain <- centralTrain %>%
  mutate(Re = case_when(
    Re == 398 ~ "High",
    Re == 224 ~ "Medium",
    Re == 90 ~ "Low")) %>%
  mutate(Re = factor(Re, levels = c("Low", "Medium", "High")))

centralVal <- centralVal |> 
  mutate(Fr = case_when(
    Fr < 0.3 ~ "Low",
    Fr == 0.3 ~ "Medium",
    Fr == Inf ~ "Infinity"
  )) |> 
  mutate(Fr = factor(Fr, levels = c("Low", "Medium", "Infinity")))

centralVal <- centralVal %>%
  mutate(Re = case_when(
    Re == 398 ~ "High",
    Re == 224 ~ "Medium",
    Re == 90 ~ "Low")) %>%
  mutate(Re = factor(Re, levels = c("Low", "Medium", "High")))
```

### Simple Linear Regression Benchmark

```{r}
#| label: Baseline-Model

SLRMean <- lm(mean ~ St + Re + Fr, data = centralTrain)
yhat <- predict(SLRMean, newdata = centralVal)
MSE_SLR_Mean <- sum((yhat - centralVal$mean)^2)/nrow(centralVal)

SLRVar <- lm(variance ~ St + Re + Fr, data = centralTrain)
yhat <- predict(SLRVar, newdata = centralVal)
MSE_SLR_Var <- sum((yhat - centralVal$variance)^2)/nrow(centralVal)

SLRSkew <- lm(skewness ~ St + Re + Fr, data = centralTrain)
yhat <- predict(SLRVar, newdata = centralVal)
MSE_SLR_Skew <- sum((yhat - centralVal$skewness)^2)/nrow(centralVal)

SLRKurt <- lm(kurtosis ~ St + Re + Fr, data = centralTrain)
yhat <- predict(SLRKurt, newdata = centralVal)
MSE_SLR_Kurt <- sum((yhat - centralVal$kurtosis)^2)/nrow(centralVal)
```

```{r}
#| label: Model-Metrics-SLR

SLRModelMetrics <- tribble(~Mean, ~Variance, ~Skewness, ~Kurtosis,
                           MSE_SLR_Mean, MSE_SLR_Var, MSE_SLR_Skew, MSE_SLR_Kurt)
SLRModelMetrics |> 
  kable(caption = "Mean Squared Error using Simple Linear Regression")
```

It is clear looking at the MSEs of the moment that the using a Simple Linear Regression model to predict anything but the `mean` is not a good idea as the performance is not good, with increasingly high MSEs for each moment. Nonetheless, it serves as a good baseline for assessing further complex models.

### Polynomials

We conducted cross-validation to determine the best fitting degree-polynomial for Stokes number. Our chosen polynomial models will include some degree polynomial of Stokes number, while adding Froude and Reynolds number as well. Since these variables are factors, we cannot make them polynomials, however, we still include these variables in our model since we decided against sparse models since we only start with three predictors, and all three predictors have a significant effect on the first four moments of particle cluster volume.

```{r, warning = FALSE}
set.seed(1)
cv_error <- rep(0,10)
for(i in 1:10){
  fit <- glm(mean ~ poly(St, i, raw = TRUE), data = centralTrain)
  cv_error[i] <- cv.glm(centralTrain, fit, K = 10)$delta[1]
}
#p1 <- plot(cv_error, type = "b", xlab = "Degree (mean, St)")
```

```{r, warning = FALSE}
set.seed(1)
cv_error <- rep(0,10)
for(i in 1:10){
  fit <- glm(variance ~ poly(St, i, raw = TRUE), data = centralTrain)
  cv_error[i] <- cv.glm(centralTrain, fit, K = 10)$delta[1]
}
#p2 <- plot(cv_error, type = "b", xlab = "Degree (variance, St)")
```

```{r, warning = FALSE}
set.seed(1)
cv_error <- rep(0,10)
for(i in 1:10){
  fit <- glm(skewness ~ poly(St, i, raw = TRUE), data = centralTrain)
  cv_error[i] <- cv.glm(centralTrain, fit, K = 10)$delta[1]
}
#p3 <- plot(cv_error, type = "b", xlab = "Degree (skewness, St)")
```

```{r, warning = FALSE}
set.seed(1)
cv_error <- rep(0,10)
for(i in 1:10){
  fit <- glm(kurtosis ~ poly(St, i, raw = TRUE), data = centralTrain)
  cv_error[i] <- cv.glm(centralTrain, fit, K = 10)$delta[1]
}
#p4 <- plot(cv_error, type = "b", xlab = "Degree (kurtosis, St)")


```

The optimal degree polynomials for predicting mean, variance, skew, and kurtosis are all 2 for Stokes Number.

```{r}
poly_mean <- lm(mean ~ poly(St, 2) + Re + Fr, data = centralTrain)
yhat <- predict(poly_mean, newdata = centralVal)
MSE_Poly_Mean <- sum((yhat - centralVal$mean)^2)/nrow(centralVal)
```

```{r}
poly_var <- lm(variance ~ poly(St, 2) + Re + Fr, data = centralTrain)
yhat <- predict(poly_var, newdata = centralVal)
MSE_Poly_Var <- sum((yhat - centralVal$variance)^2)/nrow(centralVal)
```

```{r}
poly_skew <- lm(skewness ~ poly(St, 2) + Re + Fr, data = centralTrain)
yhat <- predict(poly_skew, newdata = centralVal)
MSE_Poly_Skew <- sum((yhat - centralVal$skewness)^2)/nrow(centralVal)
```

```{r}
poly_kurt <- lm(kurtosis ~ poly(St, 2) + Re + Fr, data = centralTrain)
yhat <- predict(poly_kurt, newdata = centralVal)
MSE_Poly_Kurt <- sum((yhat - centralVal$kurtosis)^2)/nrow(centralVal)


```

Using 2nd degree polynomial models while also predicting the moments with Froude and Reynolds numbers, we get lower MSEs for all four moments, however these MSEs are still relatively high, indicating that using polynomials would also not be the best way of predicting the moments.

### Interaction Effects

We check to see if there are significant interaction effects between the three predictors, and then construct a model with these interaction effects. We expect there to be significant interaction effects between the three predictors due to the similarity between the three numbers. For example, we expect Reynolds number (fluid turbulence) to have some relationship with Frouds number (gravitational acceleration).

```{r}
inter_fit_mean <- lm(mean ~ (Re + Fr + St)^2, data = centralTrain)
yhat <- predict(inter_fit_mean, newdata = centralVal)
MSE_Inter_Mean <- sum((yhat - centralVal$mean)^2)/nrow(centralVal)

inter_fit_var <- lm(variance ~ (Re + Fr + St)^2, data = centralTrain)
yhat <- predict(inter_fit_var, newdata = centralVal)
MSE_Inter_Var <- sum((yhat - centralVal$variance)^2)/nrow(centralVal)


inter_fit_skew <- lm(skewness ~ (Re + Fr + St)^2, data = centralTrain)
yhat <- predict(inter_fit_skew, newdata = centralVal)
MSE_Inter_Skew <- sum((yhat - centralVal$skewness)^2)/nrow(centralVal)


inter_fit_kurt <- lm(kurtosis ~ (Re + Fr + St)^2, data = centralTrain)
yhat <- predict(inter_fit_kurt, newdata = centralVal)
MSE_Inter_Kurt <- sum((yhat - centralVal$kurtosis)^2)/nrow(centralVal)
```

### Trees

Finally, we construct a tree-based model and a boosted model.

```{r}
tree_mean <- tree(mean ~ St + Fr + Re, data = centralTrain)
yhat <- predict(tree_mean, newdata = centralVal)
MSE_Tree_mean <- sum((yhat - centralVal$mean)^2)/nrow(centralVal)

tree_var <- tree(variance ~ St + Fr + Re, data = centralTrain)
yhat <- predict(tree_var, newdata = centralVal)
MSE_Tree_var <- sum((yhat - centralVal$variance)^2)/nrow(centralVal)


tree_skew <- tree(skewness ~ St + Fr + Re, data = centralTrain)
yhat <- predict(tree_skew, newdata = centralVal)
MSE_Tree_skew <- sum((yhat - centralVal$skewness)^2)/nrow(centralVal)


tree_kurt <- tree(kurtosis ~ St + Fr + Re, data = centralTrain)
yhat <- predict(tree_kurt, newdata = centralVal)
MSE_Tree_kurt <- sum((yhat - centralVal$kurtosis)^2)/nrow(centralVal)

```

::: {layout-nrow="2"}
```{r}
plot(tree_mean)
title("Fully Grown Tree Model for Mean")
text(tree_mean)
```

```{r}
plot(tree_var)
title("Fully Grown Tree Model for Variance")
text(tree_var)

```

```{r}
plot(tree_skew)
title("Fully Grown Tree Model for Skewness")
text(tree_skew)
```

```{r}
plot(tree_kurt)
title("Fully Grown Tree Model for Kurtosis")
text(tree_kurt)
```
:::

We tested whether pruning our tree would help predictive performance, but found that it did not. Therefore, we left all our trees as fully grown trees shown above.

### Boosting

```{r}
library(gbm)
set.seed(123)

boost_mean <- gbm(mean ~ St + Fr + Re, data = centralTrain, 
                    distribution = "gaussian", n.trees = 5000,
                    interaction.depth = 4)
yhat <- predict(boost_mean, newdata = centralVal, n.trees = 5000)
MSE_Boost_mean <- sum((yhat - centralVal$mean)^2)/nrow(centralVal)


boost_var <- gbm(variance ~ St + Fr + Re, data = centralTrain, 
                    distribution = "gaussian", n.trees = 5000,
                    interaction.depth = 4)
yhat <- predict(boost_var, newdata = centralVal, n.trees = 5000)
MSE_Boost_var <- sum((yhat - centralVal$variance)^2)/nrow(centralVal)

boost_skew <- gbm(skewness ~ St + Fr + Re, data = centralTrain, 
                    distribution = "gaussian", n.trees = 5000,
                    interaction.depth = 4)
yhat <- predict(boost_skew, newdata = centralVal, n.trees = 5000)
MSE_Boost_skew <- sum((yhat - centralVal$skewness)^2)/nrow(centralVal)

boost_kurt <- gbm(kurtosis ~ St + Fr + Re, data = centralTrain, 
                    distribution = "gaussian", n.trees = 5000,
                    interaction.depth = 4)
yhat <- predict(boost_kurt, newdata = centralVal, n.trees = 5000)
MSE_Boost_kurt <- sum((yhat - centralVal$kurtosis)^2)/nrow(centralVal)
```

## Results: Evaluating Our Model & Investigating Relationsips

```{r}
#| echo: false
ModelMetricsMoment1 <- tribble(~ModelType, ~MSE,
                        "Linear Regression", MSE_SLR_Mean,
                        "2nd Degree Polynomial", MSE_Poly_Mean,
                        "Interaction Effects", MSE_Inter_Mean,
                        "Tree Based Model", MSE_Tree_mean,
                        "Boosted Model", MSE_Boost_mean) 
ModelMetricsMoment2 <- tribble(~ModelType, ~MSE,
                        "Linear Regression", MSE_SLR_Var,
                        "2nd Degree Polynomial", MSE_Poly_Var,
                        "Interaction Effects", MSE_Inter_Var,
                        "Tree Based Model", MSE_Tree_var,
                        "Boosted Model", MSE_Boost_var) 
ModelMetricsMoment3 <- tribble(~ModelType, ~MSE,
                        "Linear Regression", MSE_SLR_Skew,
                        "2nd Degree Polynomial", MSE_Poly_Skew,
                        "Interaction Effects", MSE_Inter_Skew,
                        "Tree Based Model", MSE_Tree_skew,
                        "Boosted Model", MSE_Boost_skew) 
ModelMetricsMoment4 <- tribble(~ModelType, ~MSE,
                        "Linear Regression", MSE_SLR_Kurt,
                        "2nd Degree Polynomial", MSE_Poly_Kurt,
                        "Interaction Effects", MSE_Inter_Kurt,
                        "Tree Based Model", MSE_Tree_kurt,
                        "Boosted Model", MSE_Boost_kurt)
```

::: {layout-nrow="2"}
```{r}
ModelMetricsMoment1 %>% kable(digits = 6, caption = "Mean")
```

```{r}
ModelMetricsMoment2 %>% kable(digits = 2, caption = "Variance")
```

```{r}
ModelMetricsMoment3 %>% kable(digits = 2, caption = "Skew")
```

```{r}
ModelMetricsMoment4 %>% kable(digits = 2, caption = "Kurtosis")
```
:::

```{r}
results <- c(ModelMetricsMoment1 |> arrange(MSE) |> slice(1) |> pull(ModelType),
             ModelMetricsMoment2 |> arrange(MSE) |> slice(1) |> pull(ModelType),
             ModelMetricsMoment3 |> arrange(MSE) |> slice(1) |> pull(ModelType),
             ModelMetricsMoment4 |> arrange(MSE) |> slice(1) |> pull(ModelType))
results
```

Our final results of the MSE for each model for all of the four moments leads us to selecting the tree-based model. This is the model that has the lowest MSE for `mean`, `variance`, `skew`, and `kurtosis`. The tree-based model makes for interpretable model for a tricky set of data which included only one quantitative predictor (Stokes number) and two factor variables (Reynolds number and Frouds number). While the data supports us using a tree-based model, we understand that there comes some uncertainty with this model selection. We randomly sampled a validation set to test our error and if we sampled a different batch of data, a different model may have predicted better.

## Conclusion

### Investigating Relationships

::: {layout-nrow="2"}
```{r}
summary(SLRMean)$coefficients |> 
  as.data.frame() |> 
  kable(caption = "Mean Regression Model")
```

```{r}
summary(SLRVar)$coefficients |> 
  as.data.frame() |> 
kable(caption = "Variance Regression Model")
```

```{r}
summary(SLRSkew)$coefficients |> 
  as.data.frame() |> 
  kable(caption = "Skewness Regression Model")
```

```{r}
summary(SLRKurt)$coefficients |> 
  as.data.frame() |> 
  kable(caption = "Kurtosis Regression Model")
```
:::

For each response variable, the input variables have a different effect. For the:

-   Mean

-   Variance

-   Skewness

-   Kurtosis
