---
title: "STA325 Case Study"
author: "Chase Mathis, Dillan Sant"
format: pdf
execute: 
  warning: false
  echo: false
editor: visual
---

## Packages/Data

```{r}
#| echo: true
library(tidyverse) # datawrangling
library(tidymodels) #modeling steps
library(knitr) # nice output
library(boot)
library(patchwork)

```

```{r}
ggplot2::theme_set(ggplot2::theme_minimal())

train <- read_csv("data/data-train.csv")
test <- read_csv("data/data-test.csv")
```

```{r}

## mutate data so that we get central moments
centralTrain <- train |> 
  mutate(mean = R_moment_1,
         variance = R_moment_2 - R_moment_1^2,
         skewness = R_moment_3 -3*R_moment_1*R_moment_2 + 2*R_moment_1^3,
         kurtosis = R_moment_4 - 4*R_moment_1*R_moment_3 + 6*R_moment_2*R_moment_1^2 - 3*R_moment_1^4) |> 
  select(-contains("R_"))
```

#### Investigating Relationship Between Mean and Froude's Number

```{r}
centralTrain |> 
  count(Fr) |> 
  kable(col.names = c("Froude's Number", "Count"), caption = "Distribution of Froude's Number")
```

Similar to Reynolds number, this number is somewhat non-continuous as there is only three buckets for the number. Therefore I am going to make it a categorical variable to help interpretability. For instance, how do we interpret Infinity? Instead I will make

-   0.052 == `Low`

-   0.300 == `Medium`

-   Inf == `Infinity`

```{r}
centralTrain <- centralTrain |> 
  mutate(Fr = case_when(
    Fr < 0.3 ~ "Low",
    Fr == 0.3 ~ "Medium",
    Fr == Inf ~ "Infinity"
  )) |> 
  mutate(Fr = factor(Fr, levels = c("Low", "Medium", "Infinity")))

centralTrain <- centralTrain %>%
  mutate(Re = case_when(
    Re == 398 ~ "High",
    Re == 224 ~ "Medium",
    Re == 90 ~ "Low")) %>%
  mutate(Re = factor(Re, levels = c("Low", "Medium", "High")))

centralTrain |> 
  ggplot(aes(x = mean, fill = Fr)) + 
  geom_histogram(bins = 8) + 
  facet_wrap(~Fr) + 
  theme(axis.text.x = element_text(angle = 45)) + 
  labs(
    x = "Raw Moment 1",
    y = "Count",
    fill = "Froud's Number",
    title = "Relationship between Froud's Number and the Mean"
  )
```

It seems hard to distinguish what affect Froud's number has on the mean from this plot.

## Simple Linear Regression

```{r}
#| label: Baseline-Model
SLRMean <- lm(mean ~ St + Re + Fr, data = centralTrain)
MSE_SLR_Mean <- sum(resid(SLRMean)^2)/nrow(centralTrain)
```

```{r}
SLRVar <- lm(variance ~ St + Re + Fr, data = centralTrain)
MSE_SLR_Var <- sum(resid(SLRVar)^2)/nrow(centralTrain)
```

```{r}
SLRSkew <- lm(skewness ~ St + Re + Fr, data = centralTrain)
MSE_SLR_Skew <- sum(resid(SLRSkew)^2)/nrow(centralTrain)
```

```{r}
SLRKurt <- lm(kurtosis ~ St + Re + Fr, data = centralTrain)
MSE_SLR_Kurt <- sum(resid(SLRKurt)^2)/nrow(centralTrain)
```

```{r}
#| label: Model-Metrics-SLR

SLRModelMetrics <- tribble(~Mean, ~Variance, ~Skewness, ~Kurtosis,
                           MSE_SLR_Mean, MSE_SLR_Var, MSE_SLR_Skew, MSE_SLR_Kurt)

SLRModelMetrics |> 
  kable(caption = "Mean Squared Error using Simple Linear Regression")
```

It is clear that the using a Simple Linear Regression model to predict anything but the `mean` is not a good idea as the performance is not good. Nonetheless it serves as a good baseline for predicting further complex models.

## Polynomials

```{r}
poly_mean <- lm(mean ~ poly(St, 2) + Re + Fr, data = centralTrain)
mse_poly_mean <- sum(resid(poly_mean)^2)/nrow(centralTrain)
```

```{r}
poly_var <- lm(variance ~ poly(St, 2) + Re + Fr, data = centralTrain)
mse_poly_var <- sum(resid(poly_var)^2)/nrow(centralTrain)
```

```{r}
poly_skew <- lm(skewness ~ poly(St, 2) + Re + Fr, data = centralTrain)
mse_poly_skew <- sum(resid(poly_skew)^2)/nrow(centralTrain)
```

```{r}
poly_kurt <- lm(kurtosis ~ poly(St, 2) + Re + Fr, data = centralTrain)
mse_poly_kurt <- sum(resid(poly_kurt)^2)/nrow(centralTrain)
```

Using 2nd degree polynomial models while also predicting the moments with Froud's and Reynolds' Numnbers, we get lower MSE's for all four moments, however these MSEs are still outrageously high for skew and kurtosis, indicating that using polynomials would also not be the best way of predicting the moments.

## Inference

### Stokes Number `St`

Stoke's Number quantifies the particle's characteristics i.e. it's size and density. Let's see how this parameter effects the distribution of the final turbulent space.

```{r}
StokesLMMean <- lm(mean ~ St, data = centralTrain)
summary(StokesLMMean)$coefficients
```

Using a linear model with only Stoke's predictor, we see that the predictor is statistically signifcant for an alpha level of `0.01`. We can interpret it's coefficient as for every one point increase in Stoke's number, the `mean` of the final turbulent distribution increases by about `0.015` .

```{r}
StokesLmVar <- lm(variance ~ St, data = centralTrain)
print("Variance Output")
summary(StokesLmVar)$coefficients
StokesLmSkew <- lm(skewness ~ St, data = centralTrain)
print("Skewness Output")
summary(StokesLmSkew)$coefficients
StokesLmKurt <- lm(kurtosis ~ St, data = centralTrain)
print("Kurtosis Output")
summary(StokesLmKurt)$coefficients
```

Looking at the summary for a Linear Model predicting the variance of the final distribution with Stoke's Number, we can see that it has essentially no affect on the `variance`, `skewness`, or `kurtosis` of the final system. Overall, this parameter does not seem to have a great impact on the final resulting distribution in general. This can be made clear by visualizing it's relationship:

```{r}
p1 <- ggplot(data = centralTrain, aes(x = St, y = mean)) + geom_point() + 
  labs(
    x = "Stoke's Number",
    y = "Mean"
  )
p2 <- ggplot(data = centralTrain, aes(x = St, y = variance)) + geom_point()+ 
  labs(
    x = "Stoke's Number",
    y = "Variance"
  )
p3 <- ggplot(data = centralTrain, aes(x = St, y = skewness)) + geom_point()+ 
  labs(
    x = "Stoke's Number",
    y = "Skewness"
  )
p4 <- ggplot(data = centralTrain, aes(x = St, y = kurtosis)) + geom_point()+ 
  labs(
    x = "Stoke's Number",
    y = "Kurtosis"
  )

(p1 + p2)/(p3 + p4)
```

### Reynold's Number `Re`

Reynold's Number `Re` in our data is a number that quantifies the initial fluid turbulence of the system.

#### `Mean` Model

```{r}
Re_LM_Mean <- lm(mean ~ Re, data = centralTrain)
summary(Re_LM_Mean)$coefficients 
```

Looking at the output, it is clear that `Re` has a statistically significant relationship with the outputting mean of the distribution for each of its levels. However, the true effect is not very large. However, we can say that when `Re` == `Low` , the resulting mean is 0.11 points higher on average than `Medium` or `High`. When `Re` == `Medium` and `Re` == `High` the resulting mean is around 11 points **lower** than the baseline when `Re` == `Low`. Therefore, it is clear to see that `Re` has an **inverse** relationship with the mean of the resulting distribution.

#### `Variance` Model

```{r}
Re_LM_Variance <- lm(variance ~ Re, data = centralTrain)
summary(Re_LM_Variance)$coefficients 
```

Similar to the `Mean` model, if we glance at the output, it is clear that `Re` has a statistically significant relationship with the outputting mean of the distribution for each of its levels. The true effect in this case is relatively large. When `Re` == `Low`, the resulting variance is 265.45 times higher on average than `Medium` or `High`. When `Re` == `Medium` or `Re` == `High` the resulting variance is around 265 times **lower** than the baseline when `Re` == `Low`. Therefore, it is clear to see that `Re` has an **inverse** relationship with the `variance` of the resulting distribution. In other words, **a higher Reynold's number results in a lower varied distribution.**

This follows the intutition that when a gravitational force is higher, clusters will form at a higher rate resulting in a lower variance number.

### Simple Linear Regression

```{r}
#| label: Baseline-Model
SLRMean <- lm(mean ~ St + Re + Fr, data = centralTrain)
MSE_SLR_Mean <- sum(resid(SLRMean)^2)/nrow(centralTrain)
```

```{r}
SLRVar <- lm(variance ~ St + Re + Fr, data = centralTrain)
MSE_SLR_Var <- sum(resid(SLRVar)^2)/nrow(centralTrain)
```

```{r}
SLRSkew <- lm(skewness ~ St + Re + Fr, data = centralTrain)
MSE_SLR_Skew <- sum(resid(SLRSkew)^2)/nrow(centralTrain)
```

```{r}
SLRKurt <- lm(kurtosis ~ St + Re + Fr, data = centralTrain)
MSE_SLR_Kurt <- sum(resid(SLRKurt)^2)/nrow(centralTrain)
```

```{r}
#| label: Model-Metrics-SLR

SLRModelMetrics <- tribble(~Mean, ~Variance, ~Skewness, ~Kurtosis,
                           MSE_SLR_Mean, MSE_SLR_Var, MSE_SLR_Skew, MSE_SLR_Kurt)

SLRModelMetrics |> 
  kable(caption = "Mean Squared Error using Simple Linear Regression")
```

It is clear that the using a Simple Linear Regression model to predict anything but the `mean` is not a good idea as the performance is not good. Nonetheless it serves as a good baseline for predicting further complex models.

### Polynomials


```{r, warning = FALSE}
set.seed(1)
cv_error <- rep(0,10)
for(i in 1:10){
  fit <- glm(mean ~ poly(St, i, raw = TRUE), data = centralTrain)
  cv_error[i] <- cv.glm(centralTrain, fit, K = 10)$delta[1]
}
#p1 <- plot(cv_error, type = "b", xlab = "Degree (mean, St)")
```

```{r, warning = FALSE}
set.seed(1)
cv_error <- rep(0,10)
for(i in 1:10){
  fit <- glm(variance ~ poly(St, i, raw = TRUE), data = centralTrain)
  cv_error[i] <- cv.glm(centralTrain, fit, K = 10)$delta[1]
}
#p2 <- plot(cv_error, type = "b", xlab = "Degree (variance, St)")
```

```{r, warning = FALSE}
set.seed(1)
cv_error <- rep(0,10)
for(i in 1:10){
  fit <- glm(skewness ~ poly(St, i, raw = TRUE), data = centralTrain)
  cv_error[i] <- cv.glm(centralTrain, fit, K = 10)$delta[1]
}
#p3 <- plot(cv_error, type = "b", xlab = "Degree (skewness, St)")
```

```{r, warning = FALSE}
set.seed(1)
cv_error <- rep(0,10)
for(i in 1:10){
  fit <- glm(kurtosis ~ poly(St, i, raw = TRUE), data = centralTrain)
  cv_error[i] <- cv.glm(centralTrain, fit, K = 10)$delta[1]
}
#p4 <- plot(cv_error, type = "b", xlab = "Degree (kurtosis, St)")
```

```{r}
(p1 + p2)/(p3+p4)
```

The optimal degree polynomials for predicting mean, variance, skew, and kurtosis are all 2 for Stokes Number.

```{r}
poly_mean <- lm(mean ~ poly(St, 2) + Re + Fr, data = centralTrain)
mse_poly_mean <- sum(resid(poly_mean)^2)/nrow(centralTrain)
```

```{r}
poly_var <- lm(variance ~ poly(St, 2) + Re + Fr, data = centralTrain)
mse_poly_var <- sum(resid(poly_var)^2)/nrow(centralTrain)
```

```{r}
poly_skew <- lm(skewness ~ poly(St, 2) + Re + Fr, data = centralTrain)
mse_poly_skew <- sum(resid(poly_skew)^2)/nrow(centralTrain)
```

```{r}
poly_kurt <- lm(kurtosis ~ poly(St, 2) + Re + Fr, data = centralTrain)
mse_poly_kurt <- sum(resid(poly_kurt)^2)/nrow(centralTrain)
```

Using 2nd degree polynomial models while also predicting the moments with Froud's and Reynolds' Numnbers, we get lower MSE's for all four moments, however these MSEs are still outrageously high for skew and kurtosis, indicating that using polynomials would also not be the best way of predicting the moments.

### Interaction Effects (to-do)

```{r}
inter_fit <- lm(mean ~ (Re + Fr + St)^2, data = centralTrain)
summary(inter_fit)$coefficients
```

### Splines

### Trees

### Random Forests

## Evaluating Our Models

```{r}
#| echo: false
ModelMetricsMoment1 <- tribble(~ModelType, ~MSE,
                        "Linear Regression", MSE_SLR_Mean,
                        "2nd Degree Polynomial", mse_poly_mean)
ModelMetricsMoment2 <- tribble(~ModelType, ~MSE,
                        "Linear Regression", MSE_SLR_Var,
                        "2nd Degree Polynomial", mse_poly_var)
ModelMetricsMoment3 <- tribble(~ModelType, ~MSE,
                        "Linear Regression", MSE_SLR_Skew,
                        "2nd Degree Polynomial", mse_poly_skew)
ModelMetricsMoment4 <- tribble(~ModelType, ~MSE,
                        "Linear Regression", MSE_SLR_Kurt,
                        "2nd Degree Polynomial", mse_poly_kurt)
```



## TODO:

Splines, (Natural, Cubic...), Tree based model

