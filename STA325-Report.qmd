---
title: "STA325 Case Study"
author: "Chase Mathis, Dillan Sant"
format: html
execute: 
  warning: false
  echo: false
editor: visual
---

## Packages/Data

```{r}
#| echo: true
library(tidyverse) # datawrangling
library(tidymodels) #modeling steps
library(knitr) # nice output
library(boot)
library(patchwork)

ggplot2::theme_set(ggplot2::theme_minimal())

train <- read_csv("data/data-train.csv")
test <- read_csv("data/data-test.csv")
```

```{r}
## mutate data so that we get central moments
centralTrain <- train |> 
  mutate(mean = R_moment_1,
         variance = R_moment_2 - R_moment_1^2,
         skewness = R_moment_3 -3*R_moment_1*R_moment_2 + 2*R_moment_1^3,
         kurtosis = R_moment_4 - 4*R_moment_1*R_moment_3 + 6*R_moment_2*R_moment_1^2 - 3*R_moment_1^4) |> 
  select(-contains("R_"))
```

#### Investigating Relationship Between Mean and Froude's Number

```{r}
centralTrain |> 
  count(Fr) |> 
  kable(col.names = c("Froude's Number", "Count"), caption = "Distribution of Froude's Number")
```

Similar to Reynolds number, this number is somewhat non-continuous as there is only three buckets for the number. Therefore I am going to make it a categorical variable to help interprebaility. For instance, how do we interpret Infinity? Instead I will make

-   0.052 == `Low`

-   0.300 == `Medium`

-   Inf == `Infinity`

```{r}
centralTrain <- centralTrain |> 
  mutate(Fr = case_when(
    Fr < 0.3 ~ "Low",
    Fr == 0.3 ~ "Medium",
    Fr == Inf ~ "Infinity"
  )) |> 
  mutate(Fr = factor(Fr, levels = c("Low", "Medium", "Infinity")))

centralTrain <- centralTrain %>%
  mutate(Re = case_when(
    Re == 398 ~ "High",
    Re == 224 ~ "Medium",
    Re == 90 ~ "Low")) %>%
  mutate(Re = factor(Re, levels = c("Low", "Medium", "High")))

centralTrain |> 
  ggplot(aes(x = mean, fill = Fr)) + 
  geom_histogram(bins = 8) + 
  facet_wrap(~Fr) + 
  theme(axis.text.x = element_text(angle = 45)) + 
  labs(
    x = "Raw Moment 1",
    y = "Count",
    fill = "Froud's Number",
    title = "Relationship between Froud's Number and the Mean"
  )
```

It seems hard to distinguish what affect Froud's number has on the mean from this plot.

## Simple Linear Regression

```{r}
#| label: Baseline-Model
SLRMean <- lm(mean ~ St + Re + Fr, data = centralTrain)
MSE_SLR_Mean <- sum(resid(SLRMean)^2)/nrow(centralTrain)
```

```{r}
SLRVar <- lm(variance ~ St + Re + Fr, data = centralTrain)
MSE_SLR_Var <- sum(resid(SLRVar)^2)/nrow(centralTrain)
```

```{r}
SLRSkew <- lm(skewness ~ St + Re + Fr, data = centralTrain)
MSE_SLR_Skew <- sum(resid(SLRSkew)^2)/nrow(centralTrain)
```

```{r}
SLRKurt <- lm(kurtosis ~ St + Re + Fr, data = centralTrain)
MSE_SLR_Kurt <- sum(resid(SLRKurt)^2)/nrow(centralTrain)
```

```{r}
#| label: Model-Metrics-SLR

SLRModelMetrics <- tribble(~Mean, ~Variance, ~Skewness, ~Kurtosis,
                           MSE_SLR_Mean, MSE_SLR_Var, MSE_SLR_Skew, MSE_SLR_Kurt)

SLRModelMetrics |> 
  kable(caption = "Mean Squared Error using Simple Linear Regression")
```

It is clear that the using a Simple Linear Regression model to predict anything but the `mean` is not a good idea as the performance is not good. Nonetheless it serves as a good baseline for predicting further complex models.

## Polynomials

```{r}
poly_mean <- lm(mean ~ poly(St, 2) + Re + Fr, data = centralTrain)
mse_poly_mean <- sum(resid(poly_mean)^2)/nrow(centralTrain)
```

```{r}
poly_var <- lm(variance ~ poly(St, 2) + Re + Fr, data = centralTrain)
mse_poly_var <- sum(resid(poly_var)^2)/nrow(centralTrain)
```

```{r}
poly_skew <- lm(skewness ~ poly(St, 2) + Re + Fr, data = centralTrain)
mse_poly_skew <- sum(resid(poly_skew)^2)/nrow(centralTrain)
```

```{r}
poly_kurt <- lm(kurtosis ~ poly(St, 2) + Re + Fr, data = centralTrain)
mse_poly_kurt <- sum(resid(poly_kurt)^2)/nrow(centralTrain)
```

Using 2nd degree polynomial models while also predicting the moments with Froud's and Reynolds' Numnbers, we get lower MSE's for all four moments, however these MSEs are still outrageously high for skew and kurtosis, indicating that using polynomials would also not be the best way of predicting the moments.

## Inference

### Stokes Number `St`

Stoke's Number quantifies the particle's characteristics i.e. it's size and density. Let's see how this parameter effects the distribution of the final turbulent space.

```{r}
StokesLMMean <- lm(mean ~ St, data = centralTrain)
summary(StokesLMMean)$coefficients
```

Using a linear model with only Stoke's predictor, we see that the predictor is statistically signifcant for an alpha level of `0.01`. We can interpret it's coefficient as for every one point increase in Stoke's number, the `mean` of the final turbulent distribution increases by about `0.015` .

```{r}
StokesLmVar <- lm(variance ~ St, data = centralTrain)
summary(StokesLmVar)$coefficientsStokesLmSkew <- lm(variance ~ St, data = centralTrain)
summary(StokesLmSkew)$coefficients
StokesLmKurt <- lm(kurtosis ~ St, data = centralTrain)summary(StokesLmKurt)$coefficients
```

Looking at the summary for a Linear Model predicting the variance of the final distribution with Stoke's Number, we can see that it has essentially no affect on the `variance`, `skewness`, or `kurtosis` of the final system. Overall, this parameter does not seem to have a great impact on the final resulting distribution in general. This can be made clear by visualizing it's relationship:

```{r}
p1 <- ggplot(data = centralTrain, aes(x = St, y = mean)) + geom_point() + 
  labs(
    x = "Stoke's Number",
    y = "Mean",
    title = "Relationship between Stoke's Number and the Mean"
  )
p2 <- ggplot(data = centralTrain, aes(x = St, y = variance)) + geom_point()+ 
  labs(
    x = "Stoke's Number",
    y = "Variance",
    title = "Relationship between Stoke's Number and the Variance"
  )
p3 <- ggplot(data = centralTrain, aes(x = St, y = skewness)) + geom_point()+ 
  labs(
    x = "Stoke's Number",
    y = "Skewness",
    title = "Relationship between Stoke's Number and the Skewness"
  )
p4 <- ggplot(data = centralTrain, aes(x = St, y = kurtosis)) + geom_point()+ 
  labs(
    x = "Stoke's Number",
    y = "Kurtosis",
    title = "Relationship between Stoke's Number and Kurtosis"
  )

(p1 + p2)/(p3 + p4)
```

### Reynold's Number `Re`

Reynold's Number `Re` in our data is a number that quantifies the initial fluid turbulence of the system.

```{r}
Re_LM_Mean <- lm(mean ~ Re, data = centralTrain)
summary(Re_LM_Mean)
```

## Prediction

## Random Forests

## Evaluating Our Models

```{r}
(ModelMetricsMoment1 <- tribble(~ModelType, ~MSE,
                        "Linear Regression", MSE_SLR_Mean,
                        "2nd Degree Polynomial", mse_poly_mean))
(ModelMetricsMoment2 <- tribble(~ModelType, ~MSE,
                        "Linear Regression", MSE_SLR_Var,
                        "2nd Degree Polynomial", mse_poly_var))
(ModelMetricsMoment3 <- tribble(~ModelType, ~MSE,
                        "Linear Regression", MSE_SLR_Skew,
                        "2nd Degree Polynomial", mse_poly_skew))
(ModelMetricsMoment4 <- tribble(~ModelType, ~MSE,
                        "Linear Regression", MSE_SLR_Kurt,
                        "2nd Degree Polynomial", mse_poly_kurt))
```

```{r, warning = FALSE}
set.seed(1)
cv_error <- rep(0,10)
for(i in 1:10){
  fit <- glm(mean ~ poly(St, i, raw = TRUE), data = centralTrain)
  cv_error[i] <- cv.glm(centralTrain, fit, K = 10)$delta[1]
}
plot(cv_error, type = "b", xlab = "Degree (mean, St)")
```

```{r, warning = FALSE}
set.seed(1)
cv_error <- rep(0,10)
for(i in 1:10){
  fit <- glm(variance ~ poly(St, i, raw = TRUE), data = centralTrain)
  cv_error[i] <- cv.glm(centralTrain, fit, K = 10)$delta[1]
}
plot(cv_error, type = "b", xlab = "Degree (variance, St)")
```

```{r, warning = FALSE}
set.seed(1)
cv_error <- rep(0,10)
for(i in 1:10){
  fit <- glm(skewness ~ poly(St, i, raw = TRUE), data = centralTrain)
  cv_error[i] <- cv.glm(centralTrain, fit, K = 10)$delta[1]
}
plot(cv_error, type = "b", xlab = "Degree (skewness, St)")
```

```{r, warning = FALSE}
set.seed(1)
cv_error <- rep(0,10)
for(i in 1:10){
  fit <- glm(kurtosis ~ poly(St, i, raw = TRUE), data = centralTrain)
  cv_error[i] <- cv.glm(centralTrain, fit, K = 10)$delta[1]
}
plot(cv_error, type = "b", xlab = "Degree (kurtosis, St)")
```

The optimal degree polynomials for predicting mean, variance, skew, and kurtosis are all 2 for Stokes Number.
