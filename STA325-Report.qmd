---
title: "STA325 Case Study"
author: "Chase Mathis, Dillan Sant"
format: html
execute: 
  warning: false
  echo: false
editor: visual
---

## Introduction

## Methodology

```{r}
#| echo: false
library(tidyverse) # datawrangling
library(tidymodels) #modeling steps
library(knitr) # nice output
library(boot) # cv
library(patchwork) # plots
library(splines)
library(tree) # trees
ggplot2::theme_set(ggplot2::theme_minimal())

train <- read_csv("data/data-train.csv")
test <- read_csv("data/data-test.csv")
```

```{r}

## mutate data so that we get central moments
centralTrain <- train |> 
  mutate(mean = R_moment_1,
         variance = R_moment_2 - R_moment_1^2,
         skewness = R_moment_3 -3*R_moment_1*R_moment_2 + 2*R_moment_1^3,
         kurtosis = R_moment_4 - 4*R_moment_1*R_moment_3 + 6*R_moment_2*R_moment_1^2 - 3*R_moment_1^4) |> 
  select(-contains("R_"))
```

#### Quick Transformations for our Input Variables

```{r}
centralTrain |> 
  count(Re) |> 
  bind_cols(centralTrain |> count(Fr)) |> 
  kable(col.names = c("Reynold's Number", "Count", "Freyman's Number", "Count"))
```

Reynold's and Freyman's numbers are non-continuous as there is only three buckets for the numbers. Therefore I am going to make them a categorical variable to help interpretability.

```{r}
centralTrain <- centralTrain |> 
  mutate(Fr = case_when(
    Fr < 0.3 ~ "Low",
    Fr == 0.3 ~ "Medium",
    Fr == Inf ~ "Infinity"
  )) |> 
  mutate(Fr = factor(Fr, levels = c("Low", "Medium", "Infinity")))

centralTrain <- centralTrain %>%
  mutate(Re = case_when(
    Re == 398 ~ "High",
    Re == 224 ~ "Medium",
    Re == 90 ~ "Low")) %>%
  mutate(Re = factor(Re, levels = c("Low", "Medium", "High")))


```

### Simple Linear Regression Benchmark

```{r}
#| label: Baseline-Model
SLRMean <- lm(mean ~ St + Re + Fr, data = centralTrain)
MSE_SLR_Mean <- sum(resid(SLRMean)^2)/nrow(centralTrain)
```

```{r}
SLRVar <- lm(variance ~ St + Re + Fr, data = centralTrain)
MSE_SLR_Var <- sum(resid(SLRVar)^2)/nrow(centralTrain)
```

```{r}
SLRSkew <- lm(skewness ~ St + Re + Fr, data = centralTrain)
MSE_SLR_Skew <- sum(resid(SLRSkew)^2)/nrow(centralTrain)
```

```{r}
SLRKurt <- lm(kurtosis ~ St + Re + Fr, data = centralTrain)
MSE_SLR_Kurt <- sum(resid(SLRKurt)^2)/nrow(centralTrain)
```

```{r}
#| label: Model-Metrics-SLR

SLRModelMetrics <- tribble(~Mean, ~Variance, ~Skewness, ~Kurtosis,
                           MSE_SLR_Mean, MSE_SLR_Var, MSE_SLR_Skew, MSE_SLR_Kurt)

SLRModelMetrics |> 
  kable(caption = "Mean Squared Error using Simple Linear Regression")
```

It is clear that the using a Simple Linear Regression model to predict anything but the `mean` is not a good idea as the performance is not good. Nonetheless it serves as a good baseline for predicting further complex models.

### Polynomials

```{r, warning = FALSE}
set.seed(1)
cv_error <- rep(0,10)
for(i in 1:10){
  fit <- glm(mean ~ poly(St, i, raw = TRUE), data = centralTrain)
  cv_error[i] <- cv.glm(centralTrain, fit, K = 10)$delta[1]
}
#p1 <- plot(cv_error, type = "b", xlab = "Degree (mean, St)")
```

```{r, warning = FALSE}
set.seed(1)
cv_error <- rep(0,10)
for(i in 1:10){
  fit <- glm(variance ~ poly(St, i, raw = TRUE), data = centralTrain)
  cv_error[i] <- cv.glm(centralTrain, fit, K = 10)$delta[1]
}
#p2 <- plot(cv_error, type = "b", xlab = "Degree (variance, St)")
```

```{r, warning = FALSE}
set.seed(1)
cv_error <- rep(0,10)
for(i in 1:10){
  fit <- glm(skewness ~ poly(St, i, raw = TRUE), data = centralTrain)
  cv_error[i] <- cv.glm(centralTrain, fit, K = 10)$delta[1]
}
#p3 <- plot(cv_error, type = "b", xlab = "Degree (skewness, St)")
```

```{r, warning = FALSE}
set.seed(1)
cv_error <- rep(0,10)
for(i in 1:10){
  fit <- glm(kurtosis ~ poly(St, i, raw = TRUE), data = centralTrain)
  cv_error[i] <- cv.glm(centralTrain, fit, K = 10)$delta[1]
}
#p4 <- plot(cv_error, type = "b", xlab = "Degree (kurtosis, St)")

#The optimal degree polynomials for predicting mean, variance, skew, and kurtosis are all 2 for Stokes Number.
```

```{r}
poly_mean <- lm(mean ~ poly(St, 2) + Re + Fr, data = centralTrain)
mse_poly_mean <- sum(resid(poly_mean)^2)/nrow(centralTrain)
```

```{r}
poly_var <- lm(variance ~ poly(St, 2) + Re + Fr, data = centralTrain)
mse_poly_var <- sum(resid(poly_var)^2)/nrow(centralTrain)
```

```{r}
poly_skew <- lm(skewness ~ poly(St, 2) + Re + Fr, data = centralTrain)
mse_poly_skew <- sum(resid(poly_skew)^2)/nrow(centralTrain)
```

```{r}
poly_kurt <- lm(kurtosis ~ poly(St, 2) + Re + Fr, data = centralTrain)
mse_poly_kurt <- sum(resid(poly_kurt)^2)/nrow(centralTrain)

#Using 2nd degree polynomial models while also predicting the moments with Froud's and Reynolds' Numnbers, we get lower MSE's for all four moments, however these MSEs are still outrageously high for skew and kurtosis, indicating that using polynomials would also not be the best way of predicting the moments.
```

### Interaction Effects

```{r}
inter_fit_mean <- lm(mean ~ (Re + Fr + St)^2, data = centralTrain)
mse_inter_fit_mean <- sum(resid(inter_fit_mean)^2)/nrow(centralTrain)

inter_fit_var <- lm(variance ~ (Re + Fr + St)^2, data = centralTrain)
mse_inter_fit_var <- sum(resid(inter_fit_var)^2)/nrow(centralTrain)

inter_fit_skew <- lm(skewness ~ (Re + Fr + St)^2, data = centralTrain)
mse_inter_fit_skew <- sum(resid(inter_fit_skew)^2)/nrow(centralTrain)

inter_fit_kurt <- lm(kurtosis ~ (Re + Fr + St)^2, data = centralTrain)
mse_inter_fit_kurt <- sum(resid(inter_fit_kurt)^2)/nrow(centralTrain)
```

### GAM ( feel like no good too many categorical?)

```{r}
#gam_mean <- lm(mean ~ ns(St, 4) + Fr + Re, data = centralTrain)
#gam_mean_sum(resid(gam_mean)^2)/nrow(centralTrain)
```

### Trees

```{r}
tree_mean <- tree(mean ~ St + Fr + Re, data = centralTrain)
tree_mean_mse <- sum(resid(tree_mean)^2)/nrow(centralTrain)

tree_var <- tree(variance ~ St + Fr + Re, data = centralTrain)
tree_var_mse <- sum(resid(tree_var)^2)/nrow(centralTrain)

tree_skew <- tree(skewness ~ St + Fr + Re, data = centralTrain)
tree_skew_mse <- sum(resid(tree_skew)^2)/nrow(centralTrain)

tree_kurt <- tree(kurtosis ~ St + Fr + Re, data = centralTrain)
tree_kurt_mse <- sum(resid(tree_kurt)^2)/nrow(centralTrain)
```

### Boosting

```{r}
library(gbm)
set.seed(123)

boost_mean <- gbm(mean ~ St + Fr + Re, data = centralTrain, 
                    distribution = "gaussian", n.trees = 5000,
                    interaction.depth = 4)
mean_yhat <- predict(boost_mean, newdata = centralTrain, n.trees = 5000)
mse_boost_mean <- sum((mean_yhat - centralTrain$mean)^2)/nrow(centralTrain)


boost_var <- gbm(variance ~ St + Fr + Re, data = centralTrain, 
                    distribution = "gaussian", n.trees = 5000,
                    interaction.depth = 4)
var_yhat <- predict(boost_var, newdata = centralTrain, n.trees = 5000)
mse_boost_var <- sum((var_yhat - centralTrain$variance)^2)/nrow(centralTrain)

boost_skew <- gbm(skewness ~ St + Fr + Re, data = centralTrain, 
                    distribution = "gaussian", n.trees = 5000,
                    interaction.depth = 4)
skew_yhat <- predict(boost_skew, newdata = centralTrain, n.trees = 5000)
mse_boost_skew <- sum((skew_yhat - centralTrain$skewness)^2)/nrow(centralTrain)

boost_kurt <- gbm(kurtosis ~ St + Fr + Re, data = centralTrain, 
                    distribution = "gaussian", n.trees = 5000,
                    interaction.depth = 4)
kurt_yhat <- predict(boost_kurt, newdata = centralTrain, n.trees = 5000)
mse_boost_kurt <- sum((kurt_yhat - centralTrain$kurtosis)^2)/nrow(centralTrain)
```

## Evaluating Our Models

```{r}
#| echo: false
ModelMetricsMoment1 <- tribble(~ModelType, ~MSE,
                        "Linear Regression", MSE_SLR_Mean,
                        "2nd Degree Polynomial", mse_poly_mean,
                        "Interaction Effects", mse_inter_fit_mean,
                        "Simple Tree Model", tree_mean_mse,
                        "Boosted Model", mse_boost_mean) 
ModelMetricsMoment2 <- tribble(~ModelType, ~MSE,
                        "Linear Regression", MSE_SLR_Var,
                        "2nd Degree Polynomial", mse_poly_var,
                        "Interaction Effects", mse_inter_fit_var,
                        "Tree Based Model", tree_var_mse,
                        "Boosted Model", mse_boost_var) 
ModelMetricsMoment3 <- tribble(~ModelType, ~MSE,
                        "Linear Regression", MSE_SLR_Skew,
                        "2nd Degree Polynomial", mse_poly_skew,
                        "Interaction Effects", mse_inter_fit_skew,
                        "Tree Based Model", tree_skew_mse,
                        "Boosted Model", mse_boost_skew) 
ModelMetricsMoment4 <- tribble(~ModelType, ~MSE,
                        "Linear Regression", MSE_SLR_Kurt,
                        "2nd Degree Polynomial", mse_poly_kurt,
                        "Interaction Effects", mse_inter_fit_kurt,
                        "Tree Based Model", tree_kurt_mse,
                        "Boosted Model", mse_boost_kurt) 

ModelMetricsMoment1 |> arrange(MSE) |> slice(1) |> pull(ModelType)
ModelMetricsMoment2 |> arrange(MSE) |> slice(1) |> pull(ModelType)
ModelMetricsMoment3 |> arrange(MSE) |> slice(1) |> pull(ModelType)
ModelMetricsMoment4 |> arrange(MSE) |> slice(1) |> pull(ModelType)
```
