---
title: "STA325 Case Study"
author: "Chase Mathis, Dillan Sant"
format: html
execute: 
  warning: false
  echo: false
editor: visual
---

## Introduction

Nobel Prize winning Physicist Richard Feynman said that turbulence was "the most important unsolved problem of classical physics". Although, we still unexpectedly feel bumps on airplines and have trouble truly predicting where a Hurricane will strike land, we hope to add to the field of research studying how to model turbulent systems. We position our research in two key areas:

1.  Prediction
2.  Statistical Inference

Prediction is important in the context of this problem as data from turbulent systems is very computationally expensive to obtain. Therefore, experiments using data on the evolution turbulent systems need better, quicker, cheaper methods to get data.

Statistical Inference is also important as it can help elucidate the connections between turbulence and initial settings. We hope to build a model that gives clarity on this as well.

## Methodology

```{r}
#| echo: false
library(tidyverse) # datawrangling
library(tidymodels) #modeling steps
library(knitr) # nice output
library(boot) # cv
library(lmvar) # CV
library(patchwork) # plots
library(splines)
library(tree) # trees

ggplot2::theme_set(ggplot2::theme_minimal())

train <- read_csv("data/data-train.csv")
test <- read_csv("data/data-test.csv")
#make a validation set for comparing models
set.seed(123)
split <- initial_split(train)
train <- training(split)
validation <- testing(split)
```

```{r}

## mutate data so that we get central moments
centralTrain <- train |> 
  mutate(mean = R_moment_1,
         variance = R_moment_2 - R_moment_1^2,
         skewness = R_moment_3 -3*R_moment_1*R_moment_2 + 2*R_moment_1^3,
         kurtosis = R_moment_4 - 4*R_moment_1*R_moment_3 + 6*R_moment_2*R_moment_1^2 - 3*R_moment_1^4) |> 
  dplyr::select(-contains("R_"))

centralVal <- validation |> 
  mutate(mean = R_moment_1,
         variance = R_moment_2 - R_moment_1^2,
         skewness = R_moment_3 -3*R_moment_1*R_moment_2 + 2*R_moment_1^3,
         kurtosis = R_moment_4 - 4*R_moment_1*R_moment_3 + 6*R_moment_2*R_moment_1^2 - 3*R_moment_1^4) |> 
  dplyr::select(-contains("R_"))
```

We are given `r nrow(train) + nrow(test)` data points, but we only use `r nrow(train)` data points for training our model as to not overfit our model. We will examine numerous different types of models including:

-   A simple Linear Regression Model
-   More Complex Non-Linear Regression Model
-   Tree Based Model
-   Boosted Tree Model

The data has `4` response variables and `3` input variables. Each output variable is a raw moment of a final turbulent distribution. We've converted the raw moments to central moments so as for better interpret ability. We keep the first raw moment, however.

As there are only three input variables, we will stray away from methods which look for sparse models. Our model is, already, sparse so there is no need to make it more sparse.

#### Quick Transformations for our Input Variables

```{r}
centralTrain |> 
  count(Re) |> 
  bind_cols(centralTrain |> count(Fr)) |> 
  kable(col.names = c("Reynold's Number", "Count", "Freyman's Number", "Count"))
```

Reynold's and Freyman's numbers are non-continuous as there is only three buckets for the numbers. Therefore I am going to make them a categorical variable to help interpretability.

```{r}
centralTrain <- centralTrain |> 
  mutate(Fr = case_when(
    Fr < 0.3 ~ "Low",
    Fr == 0.3 ~ "Medium",
    Fr == Inf ~ "Infinity"
  )) |> 
  mutate(Fr = factor(Fr, levels = c("Low", "Medium", "Infinity")))

centralTrain <- centralTrain %>%
  mutate(Re = case_when(
    Re == 398 ~ "High",
    Re == 224 ~ "Medium",
    Re == 90 ~ "Low")) %>%
  mutate(Re = factor(Re, levels = c("Low", "Medium", "High")))

centralVal <- centralVal |> 
  mutate(Fr = case_when(
    Fr < 0.3 ~ "Low",
    Fr == 0.3 ~ "Medium",
    Fr == Inf ~ "Infinity"
  )) |> 
  mutate(Fr = factor(Fr, levels = c("Low", "Medium", "Infinity")))

centralVal <- centralVal %>%
  mutate(Re = case_when(
    Re == 398 ~ "High",
    Re == 224 ~ "Medium",
    Re == 90 ~ "Low")) %>%
  mutate(Re = factor(Re, levels = c("Low", "Medium", "High")))
```

### Simple Linear Regression Benchmark

```{r}
#| label: Baseline-Model

SLRMean <- lm(mean ~ St + Re + Fr, data = centralTrain)
yhat <- predict(SLRMean, newdata = centralVal)
MSE_SLR_Mean <- sum((yhat - centralVal$mean)^2)/nrow(centralVal)

SLRVar <- lm(variance ~ St + Re + Fr, data = centralTrain)
yhat <- predict(SLRVar, newdata = centralVal)
MSE_SLR_Var <- sum((yhat - centralVal$variance)^2)/nrow(centralVal)

SLRSkew <- lm(skewness ~ St + Re + Fr, data = centralTrain)
yhat <- predict(SLRVar, newdata = centralVal)
MSE_SLR_Skew <- sum((yhat - centralVal$skewness)^2)/nrow(centralVal)

SLRKurt <- lm(kurtosis ~ St + Re + Fr, data = centralTrain)
yhat <- predict(SLRKurt, newdata = centralVal)
MSE_SLR_Kurt <- sum((yhat - centralVal$kurtosis)^2)/nrow(centralVal)
```

```{r}
#| label: Model-Metrics-SLR

SLRModelMetrics <- tribble(~Mean, ~Variance, ~Skewness, ~Kurtosis,
                           MSE_SLR_Mean, MSE_SLR_Var, MSE_SLR_Skew, MSE_SLR_Kurt)
SLRModelMetrics |> 
  kable(caption = "Mean Squared Error using Simple Linear Regression")
```

It is clear that the using a Simple Linear Regression model to predict anything but the `mean` is not a good idea as the performance is not good. Nonetheless it serves as a good baseline for predicting further complex models.

### Polynomials

```{r, warning = FALSE}
set.seed(1)
cv_error <- rep(0,10)
for(i in 1:10){
  fit <- glm(mean ~ poly(St, i, raw = TRUE), data = centralTrain)
  cv_error[i] <- cv.glm(centralTrain, fit, K = 10)$delta[1]
}
#p1 <- plot(cv_error, type = "b", xlab = "Degree (mean, St)")
```

```{r, warning = FALSE}
set.seed(1)
cv_error <- rep(0,10)
for(i in 1:10){
  fit <- glm(variance ~ poly(St, i, raw = TRUE), data = centralTrain)
  cv_error[i] <- cv.glm(centralTrain, fit, K = 10)$delta[1]
}
#p2 <- plot(cv_error, type = "b", xlab = "Degree (variance, St)")
```

```{r, warning = FALSE}
set.seed(1)
cv_error <- rep(0,10)
for(i in 1:10){
  fit <- glm(skewness ~ poly(St, i, raw = TRUE), data = centralTrain)
  cv_error[i] <- cv.glm(centralTrain, fit, K = 10)$delta[1]
}
#p3 <- plot(cv_error, type = "b", xlab = "Degree (skewness, St)")
```

```{r, warning = FALSE}
set.seed(1)
cv_error <- rep(0,10)
for(i in 1:10){
  fit <- glm(kurtosis ~ poly(St, i, raw = TRUE), data = centralTrain)
  cv_error[i] <- cv.glm(centralTrain, fit, K = 10)$delta[1]
}
#p4 <- plot(cv_error, type = "b", xlab = "Degree (kurtosis, St)")

#The optimal degree polynomials for predicting mean, variance, skew, and kurtosis are all 2 for Stokes Number.
```

```{r}
poly_mean <- lm(mean ~ poly(St, 2) + Re + Fr, data = centralTrain)
yhat <- predict(poly_mean, newdata = centralVal)
MSE_Poly_Mean <- sum((yhat - centralVal$mean)^2)/nrow(centralVal)
```

```{r}
poly_var <- lm(variance ~ poly(St, 2) + Re + Fr, data = centralTrain)
yhat <- predict(poly_var, newdata = centralVal)
MSE_Poly_Var <- sum((yhat - centralVal$variance)^2)/nrow(centralVal)
```

```{r}
poly_skew <- lm(skewness ~ poly(St, 2) + Re + Fr, data = centralTrain)
yhat <- predict(poly_skew, newdata = centralVal)
MSE_Poly_Skew <- sum((yhat - centralVal$skewness)^2)/nrow(centralVal)
```

```{r}
poly_kurt <- lm(kurtosis ~ poly(St, 2) + Re + Fr, data = centralTrain)
yhat <- predict(poly_kurt, newdata = centralVal)
MSE_Poly_Kurt <- sum((yhat - centralVal$kurtosis)^2)/nrow(centralVal)

#Using 2nd degree polynomial models while also predicting the moments with Froud's and Reynolds' Numnbers, we get lower MSE's for all four moments, however these MSEs are still outrageously high for skew and kurtosis, indicating that using polynomials would also not be the best way of predicting the moments.
```

### Interaction Effects

```{r}
inter_fit_mean <- lm(mean ~ (Re + Fr + St)^2, data = centralTrain)
yhat <- predict(inter_fit_mean, newdata = centralVal)
MSE_Inter_Mean <- sum((yhat - centralVal$mean)^2)/nrow(centralVal)

inter_fit_var <- lm(variance ~ (Re + Fr + St)^2, data = centralTrain)
yhat <- predict(inter_fit_var, newdata = centralVal)
MSE_Inter_Var <- sum((yhat - centralVal$variance)^2)/nrow(centralVal)


inter_fit_skew <- lm(skewness ~ (Re + Fr + St)^2, data = centralTrain)
yhat <- predict(inter_fit_skew, newdata = centralVal)
MSE_Inter_Skew <- sum((yhat - centralVal$skewness)^2)/nrow(centralVal)


inter_fit_kurt <- lm(kurtosis ~ (Re + Fr + St)^2, data = centralTrain)
yhat <- predict(inter_fit_kurt, newdata = centralVal)
MSE_Inter_Kurt <- sum((yhat - centralVal$kurtosis)^2)/nrow(centralVal)
```

### Trees

```{r}
tree_mean <- tree(mean ~ St + Fr + Re, data = centralTrain)
yhat <- predict(tree_mean, newdata = centralVal)
MSE_Tree_mean <- sum((yhat - centralVal$mean)^2)/nrow(centralVal)

tree_var <- tree(variance ~ St + Fr + Re, data = centralTrain)
yhat <- predict(tree_var, newdata = centralVal)
MSE_Tree_var <- sum((yhat - centralVal$variance)^2)/nrow(centralVal)

tree_skew <- tree(skewness ~ St + Fr + Re, data = centralTrain)
yhat <- predict(tree_skew, newdata = centralVal)
MSE_Tree_skew <- sum((yhat - centralVal$skewness)^2)/nrow(centralVal)

tree_kurt <- tree(kurtosis ~ St + Fr + Re, data = centralTrain)
yhat <- predict(tree_kurt, newdata = centralVal)
MSE_Tree_kurt <- sum((yhat - centralVal$kurtosis)^2)/nrow(centralVal)
```

### Boosting

```{r}
library(gbm)
set.seed(123)

boost_mean <- gbm(mean ~ St + Fr + Re, data = centralTrain, 
                    distribution = "gaussian", n.trees = 5000,
                    interaction.depth = 4)
yhat <- predict(boost_mean, newdata = centralVal, n.trees = 5000)
MSE_Boost_mean <- sum((yhat - centralVal$mean)^2)/nrow(centralVal)


boost_var <- gbm(variance ~ St + Fr + Re, data = centralTrain, 
                    distribution = "gaussian", n.trees = 5000,
                    interaction.depth = 4)
yhat <- predict(boost_var, newdata = centralVal, n.trees = 5000)
MSE_Boost_var <- sum((yhat - centralVal$variance)^2)/nrow(centralVal)

boost_skew <- gbm(skewness ~ St + Fr + Re, data = centralTrain, 
                    distribution = "gaussian", n.trees = 5000,
                    interaction.depth = 4)
yhat <- predict(boost_skew, newdata = centralVal, n.trees = 5000)
MSE_Boost_skew <- sum((yhat - centralVal$skewness)^2)/nrow(centralVal)

boost_kurt <- gbm(kurtosis ~ St + Fr + Re, data = centralTrain, 
                    distribution = "gaussian", n.trees = 5000,
                    interaction.depth = 4)
yhat <- predict(boost_kurt, newdata = centralVal, n.trees = 5000)
MSE_Boost_kurt <- sum((yhat - centralVal$kurtosis)^2)/nrow(centralVal)
```

## Evaluating Our Models

```{r}
#| echo: false
ModelMetricsMoment1 <- tribble(~ModelType, ~MSE,
                        "Linear Regression", MSE_SLR_Mean,
                        "2nd Degree Polynomial", MSE_Poly_Mean,
                        "Interaction Effects", MSE_Inter_Mean,
                        "Tree Based Model", MSE_Tree_mean,
                        "Boosted Model", MSE_Boost_mean) 
ModelMetricsMoment2 <- tribble(~ModelType, ~MSE,
                        "Linear Regression", MSE_SLR_Var,
                        "2nd Degree Polynomial", MSE_Poly_Var,
                        "Interaction Effects", MSE_Inter_Var,
                        "Tree Based Model", MSE_Tree_var,
                        "Boosted Model", MSE_Boost_var) 
ModelMetricsMoment3 <- tribble(~ModelType, ~MSE,
                        "Linear Regression", MSE_SLR_Skew,
                        "2nd Degree Polynomial", MSE_Poly_Skew,
                        "Interaction Effects", MSE_Inter_Skew,
                        "Tree Based Model", MSE_Tree_skew,
                        "Boosted Model", MSE_Boost_skew) 
ModelMetricsMoment4 <- tribble(~ModelType, ~MSE,
                        "Linear Regression", MSE_SLR_Kurt,
                        "2nd Degree Polynomial", MSE_Poly_Kurt,
                        "Interaction Effects", MSE_Inter_Kurt,
                        "Tree Based Model", MSE_Tree_kurt,
                        "Boosted Model", MSE_Boost_kurt) 

ModelMetricsMoment1 |> arrange(MSE) |> slice(1) |> pull(ModelType)
ModelMetricsMoment2 |> arrange(MSE) |> slice(1) |> pull(ModelType)
ModelMetricsMoment3 |> arrange(MSE) |> slice(1) |> pull(ModelType)
ModelMetricsMoment4 |> arrange(MSE) |> slice(1) |> pull(ModelType)
```

The final model selected for predicting all four moments is our tree-based model. This is the model that has the lowest MSE for mean, variance, skew, and kurtosis. The tree-based model makes for interpretable model for a tricky set of data which included only one quantitative predictor (Stokes number) and two factor variables (Reynolds number and Frouds number). Our only fear is that this tree-based model might be prone to overfitting to the training data, and perhaps would have a high test error. However, we believe that due to the simplicity of the training data combined with having the lowest training MSE relative to our other models that the tree-based model serves as the best predictor of the first four central moments of particle cluster volume.  
