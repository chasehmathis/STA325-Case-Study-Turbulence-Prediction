---
title: "STA325 Case Study"
author: "Chase Mathis"
format: pdf
execute: 
  warning: false
  echo: false
editor: visual
---

## Packages/Data

```{r}
#| echo: true
library(tidyverse) # datawrangling
library(tidymodels) #modeling steps
library(knitr) # nice output
library(boot)

ggplot2::theme_set(ggplot2::theme_minimal())

train <- read_csv("data/data-train.csv")
test <- read_csv("data/data-test.csv")
```

```{r}
## mutate data so that we get central moments
centralTrain <- train |> 
  mutate(mean = R_moment_1,
         variance = R_moment_2 - R_moment_1^2,
         skewness = R_moment_3 -3*R_moment_1*R_moment_2 + 2*R_moment_1^3,
         kurtosis = R_moment_4 - 4*R_moment_1*R_moment_3 + 6*R_moment_2*R_moment_1^2 - 3*R_moment_1^4) |> 
  select(-contains("R_"))
```

## Model for the Mean

As there are multiple response variables, I believe making individual models for each one will work best for both prediction and inference. We start with some EDA.

### Exploratory Data Analysis

```{r}
centralTrain |> 
  ggplot(aes(x = mean)) + 
  geom_histogram(bins = 20) + 
  labs(
    x = "Mean",
    y = "count",
    title = "Distribution of our Response Variable: The Mean"
  )


```

The majority of the first raw moments, i.e. the mean, are very close to zero. This may impact our model, and is something to look out for.

#### Investigating Relationship Between Reynold's and Mean

```{r}
centralTrain |> 
  count(Re) |> 
  kable(col.names = c("Re", "Count"), caption = "Re Distribution")
centralTrain |> 
  ggplot(aes(x = Re, y = mean)) + 
  geom_point() + 
  labs(
    x = "Reynold's Number",
    y = "Mean",
    title = "Relationship Between Reynold's Number and the Mean"
  )
```

There are three different values for Reynolds number as seen in the table. Then, when looking at the visualization, it looks like if Reynolds number is the two larger numbers, the mean is very close to zero.

#### Investigating Relationship Between Stokes Number and Mean

```{r}
hist(centralTrain$St, main = "Histogram of Stokes Number", xlab = "Stokes Number")
centralTrain |> 
  ggplot(aes(x = St, y = mean)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) + 
    labs(
    x = "Stoke's Number",
    y = "Mean",
    title = "Relationship Between ONLY Stoke's Number and the Mean"
  )

centralTrain |> 
  ggplot(aes(x = St, y = mean, group = Re)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
    labs(
    x = "Stoke's Number",
    y = "Mean",
    title = "Relationship Between Stoke's Number and the Mean and Reynold's Number",
    group = "Reynold's Number"
  )

```

If we fit a linear model with only Stoke's Number, it looks pretty terrible, but if we add in Reynolds number it becomes much better. We noticed a small trend in the plot above, but adding Stokes number takes away some uncertainty and explains when Reynolds number is small.

#### Investigating Relationship Between Mean and Froude's Number

```{r}
centralTrain |> 
  count(Fr) |> 
  kable(col.names = c("Froude's Number", "Count"), caption = "Distribution of Froude's Number")
```

Similar to Reynolds number, this number is somewhat non-continuous as there is only three buckets for the number. Therefore I am going to make it a categorical variable to help interprebaility. For instance, how do we interpret Infinity? Instead I will make

-   0.052 == `Low`

-   0.300 == `Medium`

-   Inf == `Infinity`

```{r}
centralTrain <- centralTrain |> 
  mutate(Fr = case_when(
    Fr < 0.3 ~ "Low",
    Fr == 0.3 ~ "Medium",
    Fr == Inf ~ "Infinity"
  )) |> 
  mutate(Fr = factor(Fr, levels = c("Low", "Medium", "Infinity")))

centralTrain |> 
  ggplot(aes(x = mean, fill = Fr)) + 
  geom_histogram(bins = 8) + 
  facet_wrap(~Fr) + 
  theme(axis.text.x = element_text(angle = 45)) + 
  labs(
    x = "Raw Moment 1",
    y = "Count",
    fill = "Froud's Number",
    title = "Relationship between Froud's Number and the Mean"
  )
```

It seems hard to distinguish what affect Froud's number has on the mean from this plot.

## Simple Linear Regression

```{r}
#| label: Baseline-Model
SLRMean <- lm(mean ~ St + Re + Fr, data = centralTrain)
MSE_SLR_Mean <- sum(resid(SLRMean)^2)/nrow(centralTrain)
```

```{r}
SLRVar <- lm(variance ~ St + Re + Fr, data = centralTrain)
MSE_SLR_Var <- sum(resid(SLRVar)^2)/nrow(centralTrain)
```

```{r}
SLRSkew <- lm(skewness ~ St + Re + Fr, data = centralTrain)
MSE_SLR_Skew <- sum(resid(SLRSkew)^2)/nrow(centralTrain)
```

```{r}
SLRKurt <- lm(kurtosis ~ St + Re + Fr, data = centralTrain)
MSE_SLR_Kurt <- sum(resid(SLRKurt)^2)/nrow(centralTrain)
```

```{r}
#| label: Model-Metrics-SLR

SLRModelMetrics <- tribble(~Mean, ~Variance, ~Skewness, ~Kurtosis,
                           MSE_SLR_Mean, MSE_SLR_Var, MSE_SLR_Skew, MSE_SLR_Kurt)

SLRModelMetrics |> 
  kable(caption = "Mean Squared Error using Simple Linear Regression")
```

It is clear that the using a Simple Linear Regression model to predict anything but the `mean` is not a good idea as the performance is not good. Nonetheless it serves as a good baseline for predicting further complex models.

## Inference

### Stokes Number `St`

Stoke's Number quantifies the particle's characteristics i.e. it's size and density. Let's see how this parameter effects the distribution of the final turbulent space.

```{r}
StokesLM <- lm(mean ~ St, data = centralTrain)
summary(StokesLM)
```

Using a linear model with only Stoke's predictor

## Prediction

## Random Forests

## Evaluating Our Models

```{r}
ModelMetricsMoment1 <- tribble(~ModelType, ~MSE,
                        "Linear Regression", MSE_SLR)
#ModelMetricsMoment2
```

```{r, warning = FALSE}
set.seed(1)
cv_error <- rep(0,10)
for(i in 1:10){
  fit <- glm(mean ~ poly(St, i, raw = TRUE) + poly(Re, i, raw = TRUE) + Fr, data = centralTrain)
  cv_error[i] <- cv.glm(centralTrain, fit, K = 10)$delta[1]
}
plot(cv_error, type = "b", xlab = "Degree (mean)")
cv_error <- rep(0,10)
for(i in 1:10){
  fit <- glm(variance ~ poly(St, i, raw = TRUE) + poly(Re, i, raw = TRUE) + Fr, data = centralTrain)
  cv_error[i] <- cv.glm(centralTrain, fit, K = 10)$delta[1]
}
plot(cv_error, type = "b", xlab = "Degree (variance)")
cv_error <- rep(0,10)
for(i in 1:10){
  fit <- glm(skewness ~ poly(St, i, raw = TRUE) + poly(Re, i, raw = TRUE) + Fr, data = centralTrain)
  cv_error[i] <- cv.glm(centralTrain, fit, K = 10)$delta[1]
}
plot(cv_error, type = "b", xlab = "Degree (skewness)")
cv_error <- rep(0,10)
for(i in 1:10){
  fit <- glm(kurtosis ~ poly(St, i, raw = TRUE) + poly(Re, i, raw = TRUE) + Fr, data = centralTrain)
  cv_error[i] <- cv.glm(centralTrain, fit, K = 10)$delta[1]
}
plot(cv_error, type = "b", xlab = "Degree (kurtosis)")
```

The optimal degree polynomials for predicting mean, variance, skew, and kurtosis are 2, 2, 4, and 3, respectively.
